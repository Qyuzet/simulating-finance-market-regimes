{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting and Simulating Financial Market Regimes Using CGAN and LSTM\n",
        "\n",
        "**Authors:** Riki Awal Syahputra, Darrus Loamayer, Yiyang Liu  \n",
        "**Course:** COMP6784001 - Fundamentals of Data Science  \n",
        "**Institution:** Binus International University\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup & Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version: 2.5.1+cu121\n",
            "CUDA Available: True\n",
            "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
            "================================================================================\n",
            "FINANCIAL MARKET REGIME PREDICTION - CGAN + LSTM\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Complete Analysis Script for Financial Market Regime Prediction\n",
        "CGAN + LSTM Hybrid Model\n",
        "\n",
        "Run this script to execute the entire analysis pipeline.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Data acquisition\n",
        "import yfinance as yf\n",
        "try:\n",
        "    from fredapi import Fred\n",
        "    HAS_FRED_API = True\n",
        "except:\n",
        "    HAS_FRED_API = False\n",
        "    \n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Time series\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from scipy.stats import probplot\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# Utilities\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Random seeds\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Create directories\n",
        "Path('data/raw').mkdir(parents=True, exist_ok=True)\n",
        "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
        "Path('models').mkdir(parents=True, exist_ok=True)\n",
        "Path('results/figures').mkdir(parents=True, exist_ok=True)\n",
        "Path('results/metrics').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FINANCIAL MARKET REGIME PREDICTION - CGAN + LSTM\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 2: Data Acquisition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[1/11] DATA ACQUISITION\n",
            "Fetching S&P 500 data (2010-01-01 to 2024-12-31)...\n",
            "[OK] S&P 500: 3773 observations\n",
            "Fetching FRED data (Fed Funds Rate, CPI)...\n",
            "[OK] Fed Funds Rate: 180 observations\n",
            "[OK] CPI: 180 observations\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n[1/11] DATA ACQUISITION\")\n",
        "\n",
        "START_DATE = '2010-01-01'\n",
        "END_DATE = '2024-12-31'\n",
        "FRED_API_KEY = \"6471be419152257e21225e7de5e915c5\"  # FRED API key\n",
        "\n",
        "print(f\"Fetching S&P 500 data ({START_DATE} to {END_DATE})...\")\n",
        "sp500 = yf.download('^GSPC', start=START_DATE, end=END_DATE, progress=False)\n",
        "# Handle MultiIndex columns from yfinance\n",
        "if isinstance(sp500.columns, pd.MultiIndex):\n",
        "    sp500.columns = sp500.columns.get_level_values(0)\n",
        "sp500 = sp500[['Close']].rename(columns={'Close': 'SP500'})\n",
        "sp500.index.name = 'Date'\n",
        "sp500.to_csv('data/raw/sp500.csv', index=True)\n",
        "print(f\"[OK] S&P 500: {len(sp500)} observations\")\n",
        "\n",
        "print(\"Fetching FRED data (Fed Funds Rate, CPI)...\")\n",
        "try:\n",
        "    if FRED_API_KEY and HAS_FRED_API:\n",
        "        fred = Fred(api_key=FRED_API_KEY)\n",
        "        fedfunds = fred.get_series('FEDFUNDS', observation_start=START_DATE, observation_end=END_DATE)\n",
        "        cpi = fred.get_series('CPIAUCSL', observation_start=START_DATE, observation_end=END_DATE)\n",
        "    else:\n",
        "        fedfunds = pdr.DataReader('FEDFUNDS', 'fred', START_DATE, END_DATE).squeeze()\n",
        "        cpi = pdr.DataReader('CPIAUCSL', 'fred', START_DATE, END_DATE).squeeze()\n",
        "    \n",
        "    fedfunds.index.name = 'Date'\n",
        "    cpi.index.name = 'Date'\n",
        "    fedfunds.to_csv('data/raw/fedfunds.csv', header=True)\n",
        "    cpi.to_csv('data/raw/cpi.csv', header=True)\n",
        "    print(f\"[OK] Fed Funds Rate: {len(fedfunds)} observations\")\n",
        "    print(f\"[OK] CPI: {len(cpi)} observations\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching FRED data: {e}\")\n",
        "    print(\"Please check your internet connection or FRED API key\")\n",
        "    exit(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 3: Data Preprocessing & Feature Engineering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[2/11] DATA PREPROCESSING & FEATURE ENGINEERING\n",
            "Combined dataset shape: (0, 3)\n",
            "Date range: nan to nan\n",
            "Engineering features...\n",
            "Features engineered. Shape: (0, 11)\n",
            "Winsorizing outliers...\n",
            "[OK] Preprocessing complete\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n[2/11] DATA PREPROCESSING & FEATURE ENGINEERING\")\n",
        "\n",
        "# Load data\n",
        "sp500 = pd.read_csv('data/raw/sp500.csv', index_col=0, parse_dates=True)\n",
        "fedfunds = pd.read_csv('data/raw/fedfunds.csv', index_col=0, parse_dates=True)\n",
        "cpi = pd.read_csv('data/raw/cpi.csv', index_col=0, parse_dates=True)\n",
        "\n",
        "# Ensure proper column names (convert Series to DataFrame if needed)\n",
        "if isinstance(fedfunds, pd.Series):\n",
        "    fedfunds = fedfunds.to_frame(name='FEDFUNDS')\n",
        "elif 'FEDFUNDS' not in fedfunds.columns:\n",
        "    fedfunds.columns = ['FEDFUNDS']\n",
        "    \n",
        "if isinstance(cpi, pd.Series):\n",
        "    cpi = cpi.to_frame(name='CPI')\n",
        "elif 'CPI' not in cpi.columns:\n",
        "    cpi.columns = ['CPI']\n",
        "\n",
        "# Merge datasets\n",
        "df = sp500.copy()\n",
        "fedfunds_daily = fedfunds.reindex(df.index).ffill()\n",
        "cpi_daily = cpi.reindex(df.index).ffill()\n",
        "df['FEDFUNDS'] = fedfunds_daily['FEDFUNDS']\n",
        "df['CPI'] = cpi_daily['CPI']\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(f\"Combined dataset shape: {df.shape}\")\n",
        "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
        "\n",
        "# Feature Engineering\n",
        "print(\"Engineering features...\")\n",
        "df['returns'] = np.log(df['SP500'] / df['SP500'].shift(1))\n",
        "df['volatility'] = df['returns'].rolling(window=30).std()\n",
        "df['MA_50'] = df['SP500'].rolling(window=50).mean()\n",
        "df['MA_200'] = df['SP500'].rolling(window=200).mean()\n",
        "df['momentum'] = df['SP500'] - df['SP500'].shift(20)\n",
        "df['price_to_MA50'] = df['SP500'] / df['MA_50']\n",
        "df['price_to_MA200'] = df['SP500'] / df['MA_200']\n",
        "df['inflation'] = df['CPI'].pct_change(periods=252)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(f\"Features engineered. Shape: {df.shape}\")\n",
        "\n",
        "# Winsorization\n",
        "print(\"Winsorizing outliers...\")\n",
        "for col in ['returns', 'volatility', 'momentum']:\n",
        "    lower = df[col].quantile(0.01)\n",
        "    upper = df[col].quantile(0.99)\n",
        "    df[col] = df[col].clip(lower, upper)\n",
        "\n",
        "print(\"[OK] Preprocessing complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 4: Regime Labeling\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[3/11] REGIME LABELING\n",
            "Regime distribution:\n",
            "Series([], Name: count, dtype: int64)\n",
            "\n",
            "Regime percentages:\n",
            "Series([], Name: proportion, dtype: float64)\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index 0 is out of bounds for axis 0 with size 0",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Visualize\u001b[39;00m\n\u001b[0;32m     27\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m---> 28\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m, ax\u001b[38;5;241m=\u001b[39max, color\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     29\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarket Regime Distribution (2010-2024)\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m, fontweight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbold\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\plotting\\_core.py:1030\u001b[0m, in \u001b[0;36mPlotAccessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             label_name \u001b[38;5;241m=\u001b[39m label_kw \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   1028\u001b[0m             data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m label_name\n\u001b[1;32m-> 1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plot_backend\u001b[38;5;241m.\u001b[39mplot(data, kind\u001b[38;5;241m=\u001b[39mkind, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\__init__.py:71\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(data, kind, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124max\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ax, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_ax\u001b[39m\u001b[38;5;124m\"\u001b[39m, ax)\n\u001b[0;32m     70\u001b[0m plot_obj \u001b[38;5;241m=\u001b[39m PLOT_CLASSES[kind](data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 71\u001b[0m plot_obj\u001b[38;5;241m.\u001b[39mgenerate()\n\u001b[0;32m     72\u001b[0m plot_obj\u001b[38;5;241m.\u001b[39mdraw()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plot_obj\u001b[38;5;241m.\u001b[39mresult\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:508\u001b[0m, in \u001b[0;36mMPLPlot.generate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_plot_logic_common(ax)\n\u001b[1;32m--> 508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_plot_logic(ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:1966\u001b[0m, in \u001b[0;36mBarPlot._post_plot_logic\u001b[1;34m(self, ax, data)\u001b[0m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1964\u001b[0m     str_index \u001b[38;5;241m=\u001b[39m [pprint_thing(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m-> 1966\u001b[0m s_edge \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39max_pos[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlim_offset\n\u001b[0;32m   1967\u001b[0m e_edge \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39max_pos[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbar_width \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlim_offset\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decorate_ticks(ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_index_name(), str_index, s_edge, e_edge)\n",
            "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"\\n[3/11] REGIME LABELING\")\n",
        "\n",
        "def classify_regime(row, vol_threshold):\n",
        "    if row['volatility'] < vol_threshold and row['momentum'] > 0:\n",
        "        return 'bull'\n",
        "    elif row['volatility'] >= vol_threshold and row['momentum'] < 0:\n",
        "        return 'bear'\n",
        "    else:\n",
        "        return 'volatile'\n",
        "\n",
        "vol_threshold = df['volatility'].median()\n",
        "df['regime'] = df.apply(lambda row: classify_regime(row, vol_threshold), axis=1)\n",
        "\n",
        "# Encode regimes\n",
        "label_encoder = LabelEncoder()\n",
        "df['regime_encoded'] = label_encoder.fit_transform(df['regime'])\n",
        "\n",
        "with open('models/label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"Regime distribution:\")\n",
        "print(df['regime'].value_counts())\n",
        "print(f\"\\nRegime percentages:\")\n",
        "print(df['regime'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "df['regime'].value_counts().plot(kind='bar', ax=ax, color=['green', 'red', 'orange'])\n",
        "ax.set_title('Market Regime Distribution (2010-2024)', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Regime')\n",
        "ax.set_ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/regime_distribution.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(\"[OK] Regime labeling complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 5: Exploratory Data Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[4/11] EXPLORATORY DATA ANALYSIS\")\n",
        "\n",
        "# Time series plot with regimes\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
        "\n",
        "for regime, color in zip(['bull', 'bear', 'volatile'], ['green', 'red', 'orange']):\n",
        "    mask = df['regime'] == regime\n",
        "    axes[0].scatter(df[mask].index, df[mask]['SP500'], c=color, label=regime.capitalize(), alpha=0.5, s=1)\n",
        "axes[0].set_ylabel('S&P 500 Price')\n",
        "axes[0].set_title('S&P 500 with Market Regimes', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(df.index, df['volatility'], color='purple', linewidth=0.8)\n",
        "axes[1].set_ylabel('Volatility (30-day)')\n",
        "axes[1].set_title('Rolling Volatility', fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(df.index, df['FEDFUNDS'], color='blue', linewidth=0.8)\n",
        "axes[2].set_ylabel('Fed Funds Rate (%)')\n",
        "axes[2].set_xlabel('Date')\n",
        "axes[2].set_title('Federal Funds Rate', fontweight='bold')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/eda_timeseries.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Correlation matrix\n",
        "features_for_corr = ['returns', 'volatility', 'momentum', 'FEDFUNDS', 'inflation', 'price_to_MA50']\n",
        "corr_matrix = df[features_for_corr].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/correlation_matrix.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Distribution of returns by regime\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "for regime in ['bull', 'bear', 'volatile']:\n",
        "    df[df['regime'] == regime]['returns'].hist(bins=50, alpha=0.6, label=regime.capitalize(), ax=ax)\n",
        "ax.set_xlabel('Daily Returns')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_title('Return Distribution by Regime', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/returns_by_regime.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(\"[OK] EDA complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 6: Data Splitting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[5/11] DATA SPLITTING\")\n",
        "\n",
        "train_size = int(0.7 * len(df))\n",
        "val_size = int(0.15 * len(df))\n",
        "\n",
        "train_df = df.iloc[:train_size].copy()\n",
        "val_df = df.iloc[train_size:train_size+val_size].copy()\n",
        "test_df = df.iloc[train_size+val_size:].copy()\n",
        "\n",
        "print(f\"Train: {len(train_df)} samples ({train_df.index.min()} to {train_df.index.max()})\")\n",
        "print(f\"Val:   {len(val_df)} samples ({val_df.index.min()} to {val_df.index.max()})\")\n",
        "print(f\"Test:  {len(test_df)} samples ({test_df.index.min()} to {test_df.index.max()})\")\n",
        "\n",
        "# Feature selection\n",
        "feature_cols = ['returns', 'volatility', 'momentum', 'FEDFUNDS', 'inflation', 'price_to_MA50', 'price_to_MA200']\n",
        "target_col = 'regime_encoded'\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(train_df[feature_cols])\n",
        "X_val = scaler.transform(val_df[feature_cols])\n",
        "X_test = scaler.transform(test_df[feature_cols])\n",
        "\n",
        "y_train = train_df[target_col].values\n",
        "y_val = val_df[target_col].values\n",
        "y_test = test_df[target_col].values\n",
        "\n",
        "with open('models/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(\"[OK] Data splitting complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 7: Baseline Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[6/11] BASELINE MODELS\")\n",
        "\n",
        "baseline_metrics = {}\n",
        "\n",
        "# 6.1 ARIMA\n",
        "print(\"Training ARIMA...\")\n",
        "arima_model = ARIMA(train_df['SP500'], order=(5, 1, 0))\n",
        "arima_fitted = arima_model.fit()\n",
        "arima_forecast = arima_fitted.forecast(steps=len(test_df))\n",
        "\n",
        "arima_rmse = np.sqrt(mean_squared_error(test_df['SP500'], arima_forecast))\n",
        "arima_mae = mean_absolute_error(test_df['SP500'], arima_forecast)\n",
        "arima_r2 = r2_score(test_df['SP500'], arima_forecast)\n",
        "\n",
        "print(f\"ARIMA - RMSE: {arima_rmse:.4f}, MAE: {arima_mae:.4f}, RÂ²: {arima_r2:.4f}\")\n",
        "baseline_metrics['ARIMA'] = {'RMSE': arima_rmse, 'MAE': arima_mae, 'R2': arima_r2}\n",
        "\n",
        "with open('models/arima_model.pkl', 'wb') as f:\n",
        "    pickle.dump(arima_fitted, f)\n",
        "\n",
        "# 6.2 Logistic Regression\n",
        "print(\"Training Logistic Regression...\")\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=SEED)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_pred = lr_model.predict(X_test)\n",
        "lr_acc = accuracy_score(y_test, lr_pred)\n",
        "lr_f1 = f1_score(y_test, lr_pred, average='weighted')\n",
        "\n",
        "print(f\"Logistic Regression - Accuracy: {lr_acc:.4f}, F1: {lr_f1:.4f}\")\n",
        "baseline_metrics['LogisticRegression'] = {'Accuracy': lr_acc, 'F1': lr_f1}\n",
        "\n",
        "with open('models/logistic_regression.pkl', 'wb') as f:\n",
        "    pickle.dump(lr_model, f)\n",
        "\n",
        "# 6.3 Random Forest\n",
        "print(\"Training Random Forest...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=SEED, n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "rf_f1 = f1_score(y_test, rf_pred, average='weighted')\n",
        "\n",
        "print(f\"Random Forest - Accuracy: {rf_acc:.4f}, F1: {rf_f1:.4f}\")\n",
        "baseline_metrics['RandomForest'] = {'Accuracy': rf_acc, 'F1': rf_f1}\n",
        "\n",
        "with open('models/random_forest.pkl', 'wb') as f:\n",
        "    pickle.dump(rf_model, f)\n",
        "\n",
        "# 6.4 Monte Carlo\n",
        "print(\"Generating Monte Carlo synthetic data...\")\n",
        "n_samples = 1000\n",
        "mc_synthetic_returns = np.random.choice(train_df['returns'].values, size=n_samples, replace=True)\n",
        "\n",
        "real_hist, bins = np.histogram(test_df['returns'], bins=50, density=True)\n",
        "mc_hist, _ = np.histogram(mc_synthetic_returns, bins=bins, density=True)\n",
        "real_hist = real_hist / real_hist.sum()\n",
        "mc_hist = mc_hist / mc_hist.sum()\n",
        "mc_jsd = jensenshannon(real_hist, mc_hist)\n",
        "\n",
        "print(f\"Monte Carlo - JSD: {mc_jsd:.4f}\")\n",
        "baseline_metrics['MonteCarlo'] = {'JSD': mc_jsd}\n",
        "\n",
        "with open('results/metrics/baseline_metrics.json', 'w') as f:\n",
        "    json.dump(baseline_metrics, f, indent=2)\n",
        "\n",
        "print(\"[OK] Baseline models complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 8: CGAN Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[7/11] CGAN IMPLEMENTATION\")\n",
        "\n",
        "# CGAN Architecture\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim=100, condition_dim=9, output_dim=1):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim + condition_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(32, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, condition):\n",
        "        x = torch.cat([noise, condition], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim=1, condition_dim=9):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim + condition_dim, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, condition):\n",
        "        x = torch.cat([x, condition], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "print(\"[OK] CGAN architecture defined\")\n",
        "\n",
        "# Prepare CGAN data\n",
        "def prepare_cgan_data(df_subset, cond_scaler):\n",
        "    cond_features = ['volatility', 'momentum', 'FEDFUNDS', 'inflation', 'price_to_MA50', 'price_to_MA200']\n",
        "    conditions = cond_scaler.transform(df_subset[cond_features])\n",
        "\n",
        "    regime_onehot = np.zeros((len(df_subset), 3))\n",
        "    regime_onehot[np.arange(len(df_subset)), df_subset['regime_encoded'].values] = 1\n",
        "    conditions = np.concatenate([conditions, regime_onehot], axis=1)\n",
        "\n",
        "    returns = df_subset['returns'].values.reshape(-1, 1)\n",
        "    returns_min, returns_max = returns.min(), returns.max()\n",
        "    returns_normalized = 2 * (returns - returns_min) / (returns_max - returns_min) - 1\n",
        "\n",
        "    return torch.FloatTensor(returns_normalized), torch.FloatTensor(conditions), returns_min, returns_max\n",
        "\n",
        "# Create separate scaler for CGAN condition features\n",
        "cond_features = ['volatility', 'momentum', 'FEDFUNDS', 'inflation', 'price_to_MA50', 'price_to_MA200']\n",
        "cond_scaler = StandardScaler()\n",
        "cond_scaler.fit(train_df[cond_features])\n",
        "\n",
        "train_returns, train_conditions, ret_min, ret_max = prepare_cgan_data(train_df, cond_scaler)\n",
        "batch_size = 128\n",
        "train_dataset = TensorDataset(train_returns, train_conditions)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f\"Train returns shape: {train_returns.shape}\")\n",
        "print(f\"Train conditions shape: {train_conditions.shape}\")\n",
        "\n",
        "# Initialize CGAN\n",
        "noise_dim = 100\n",
        "condition_dim = train_conditions.shape[1]\n",
        "\n",
        "generator = Generator(noise_dim=noise_dim, condition_dim=condition_dim).to(device)\n",
        "discriminator = Discriminator(input_dim=1, condition_dim=condition_dim).to(device)\n",
        "\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "print(f\"Generator parameters: {sum(p.numel() for p in generator.parameters())}\")\n",
        "print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters())}\")\n",
        "\n",
        "# Training CGAN\n",
        "print(\"\\nTraining CGAN...\")\n",
        "num_epochs = 2000  # Reduced for time - increase to 3000-5000 for better results\n",
        "d_losses, g_losses = [], []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), desc=\"CGAN Training\"):\n",
        "    epoch_d_loss, epoch_g_loss = 0, 0\n",
        "\n",
        "    for real_data, conditions in train_loader:\n",
        "        batch_size_current = real_data.size(0)\n",
        "        real_data = real_data.to(device)\n",
        "        conditions = conditions.to(device)\n",
        "\n",
        "        real_labels = torch.ones(batch_size_current, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size_current, 1).to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        real_output = discriminator(real_data, conditions)\n",
        "        d_loss_real = criterion(real_output, real_labels)\n",
        "\n",
        "        noise = torch.randn(batch_size_current, noise_dim).to(device)\n",
        "        fake_data = generator(noise, conditions)\n",
        "        fake_output = discriminator(fake_data.detach(), conditions)\n",
        "        d_loss_fake = criterion(fake_output, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        noise = torch.randn(batch_size_current, noise_dim).to(device)\n",
        "        fake_data = generator(noise, conditions)\n",
        "        fake_output = discriminator(fake_data, conditions)\n",
        "        g_loss = criterion(fake_output, real_labels)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        epoch_g_loss += g_loss.item()\n",
        "\n",
        "    d_losses.append(epoch_d_loss / len(train_loader))\n",
        "    g_losses.append(epoch_g_loss / len(train_loader))\n",
        "\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | D Loss: {d_losses[-1]:.4f} | G Loss: {g_losses[-1]:.4f}\")\n",
        "\n",
        "print(\"[OK] CGAN training complete\")\n",
        "\n",
        "# Save models\n",
        "torch.save(generator.state_dict(), 'models/cgan_generator.pth')\n",
        "torch.save(discriminator.state_dict(), 'models/cgan_discriminator.pth')\n",
        "\n",
        "# Plot training losses\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(d_losses, label='Discriminator Loss', alpha=0.7)\n",
        "plt.plot(g_losses, label='Generator Loss', alpha=0.7)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('CGAN Training Losses', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/cgan_losses.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Generate synthetic data\n",
        "print(\"Generating synthetic data with CGAN...\")\n",
        "generator.eval()\n",
        "with torch.no_grad():\n",
        "    test_returns, test_conditions, _, _ = prepare_cgan_data(test_df, cond_scaler)\n",
        "    test_conditions = test_conditions.to(device)\n",
        "\n",
        "    noise = torch.randn(len(test_conditions), noise_dim).to(device)\n",
        "    synthetic_returns = generator(noise, test_conditions).cpu().numpy()\n",
        "    synthetic_returns = (synthetic_returns + 1) / 2 * (ret_max - ret_min) + ret_min\n",
        "\n",
        "# Calculate JSD\n",
        "real_hist, bins = np.histogram(test_df['returns'], bins=50, density=True)\n",
        "cgan_hist, _ = np.histogram(synthetic_returns, bins=bins, density=True)\n",
        "real_hist = real_hist / real_hist.sum()\n",
        "cgan_hist = cgan_hist / cgan_hist.sum()\n",
        "cgan_jsd = jensenshannon(real_hist, cgan_hist)\n",
        "\n",
        "print(f\"CGAN JSD: {cgan_jsd:.4f}\")\n",
        "print(f\"Monte Carlo JSD: {mc_jsd:.4f}\")\n",
        "print(f\"Improvement: {((mc_jsd - cgan_jsd) / mc_jsd * 100):.2f}%\")\n",
        "\n",
        "cgan_metrics = {'JSD': cgan_jsd}\n",
        "with open('results/metrics/cgan_metrics.json', 'w') as f:\n",
        "    json.dump(cgan_metrics, f, indent=2)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].hist(test_df['returns'], bins=50, alpha=0.6, label='Real', density=True, color='blue')\n",
        "axes[0].hist(synthetic_returns, bins=50, alpha=0.6, label='CGAN Synthetic', density=True, color='red')\n",
        "axes[0].hist(mc_synthetic_returns[:len(test_df)], bins=50, alpha=0.4, label='Monte Carlo', density=True, color='green')\n",
        "axes[0].set_xlabel('Returns')\n",
        "axes[0].set_ylabel('Density')\n",
        "axes[0].set_title('Return Distribution: Real vs Synthetic', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "probplot(test_df['returns'], dist=\"norm\", plot=axes[1])\n",
        "axes[1].set_title('Q-Q Plot: Real Returns', fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/synthetic_vs_real.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(\"[OK] CGAN evaluation complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 9: LSTM Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[8/11] LSTM IMPLEMENTATION\")\n",
        "\n",
        "# LSTM with Attention\n",
        "class LSTMAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, num_classes=3, dropout=0.3):\n",
        "        super(LSTMAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.attention = nn.Linear(hidden_dim, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "        out = self.fc(context)\n",
        "        return out\n",
        "\n",
        "print(\"[OK] LSTM architecture defined\")\n",
        "\n",
        "# Prepare sequence data\n",
        "def create_sequences(data, labels, seq_length=20):\n",
        "    sequences, targets = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        sequences.append(data[i:i+seq_length])\n",
        "        targets.append(labels[i+seq_length])\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "seq_length = 20\n",
        "X_train_seq, y_train_seq = create_sequences(X_train, y_train, seq_length)\n",
        "X_val_seq, y_val_seq = create_sequences(X_val, y_val, seq_length)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test, y_test, seq_length)\n",
        "\n",
        "X_train_seq = torch.FloatTensor(X_train_seq)\n",
        "y_train_seq = torch.LongTensor(y_train_seq)\n",
        "X_val_seq = torch.FloatTensor(X_val_seq)\n",
        "y_val_seq = torch.LongTensor(y_val_seq)\n",
        "X_test_seq = torch.FloatTensor(X_test_seq)\n",
        "y_test_seq = torch.LongTensor(y_test_seq)\n",
        "\n",
        "train_dataset_lstm = TensorDataset(X_train_seq, y_train_seq)\n",
        "val_dataset_lstm = TensorDataset(X_val_seq, y_val_seq)\n",
        "test_dataset_lstm = TensorDataset(X_test_seq, y_test_seq)\n",
        "\n",
        "train_loader_lstm = DataLoader(train_dataset_lstm, batch_size=64, shuffle=True)\n",
        "val_loader_lstm = DataLoader(val_dataset_lstm, batch_size=64, shuffle=False)\n",
        "test_loader_lstm = DataLoader(test_dataset_lstm, batch_size=64, shuffle=False)\n",
        "\n",
        "print(f\"Train sequences: {X_train_seq.shape}\")\n",
        "print(f\"Val sequences: {X_val_seq.shape}\")\n",
        "print(f\"Test sequences: {X_test_seq.shape}\")\n",
        "\n",
        "# Initialize LSTM\n",
        "input_dim = X_train_seq.shape[2]\n",
        "lstm_model = LSTMAttention(input_dim=input_dim, hidden_dim=64, num_layers=2, num_classes=3).to(device)\n",
        "criterion_lstm = nn.CrossEntropyLoss()\n",
        "optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "print(f\"LSTM parameters: {sum(p.numel() for p in lstm_model.parameters())}\")\n",
        "\n",
        "# Training LSTM\n",
        "print(\"\\nTraining LSTM...\")\n",
        "num_epochs_lstm = 50\n",
        "best_val_acc = 0\n",
        "patience = 10\n",
        "patience_counter = 0\n",
        "\n",
        "train_losses_lstm, val_losses_lstm = [], []\n",
        "train_accs_lstm, val_accs_lstm = [], []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs_lstm), desc=\"LSTM Training\"):\n",
        "    # Training\n",
        "    lstm_model.train()\n",
        "    train_loss, train_correct = 0, 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader_lstm:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer_lstm.zero_grad()\n",
        "        outputs = lstm_model(X_batch)\n",
        "        loss = criterion_lstm(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer_lstm.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_correct += (outputs.argmax(1) == y_batch).sum().item()\n",
        "\n",
        "    train_losses_lstm.append(train_loss / len(train_loader_lstm))\n",
        "    train_accs_lstm.append(train_correct / len(train_dataset_lstm))\n",
        "\n",
        "    # Validation\n",
        "    lstm_model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader_lstm:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = lstm_model(X_batch)\n",
        "            loss = criterion_lstm(outputs, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_correct += (outputs.argmax(1) == y_batch).sum().item()\n",
        "\n",
        "    val_losses_lstm.append(val_loss / len(val_loader_lstm))\n",
        "    val_accs_lstm.append(val_correct / len(val_dataset_lstm))\n",
        "\n",
        "    # Early stopping\n",
        "    if val_accs_lstm[-1] > best_val_acc:\n",
        "        best_val_acc = val_accs_lstm[-1]\n",
        "        torch.save(lstm_model.state_dict(), 'models/lstm_model.pth')\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs_lstm}] | Train Acc: {train_accs_lstm[-1]:.4f} | Val Acc: {val_accs_lstm[-1]:.4f}\")\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"[OK] LSTM training complete. Best val accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].plot(train_losses_lstm, label='Train Loss')\n",
        "axes[0].plot(val_losses_lstm, label='Val Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('LSTM Training Loss', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(train_accs_lstm, label='Train Accuracy')\n",
        "axes[1].plot(val_accs_lstm, label='Val Accuracy')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('LSTM Training Accuracy', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/lstm_training.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(\"[OK] LSTM implementation complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 10: Model Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[9/11] MODEL EVALUATION\")\n",
        "\n",
        "# Load best LSTM model\n",
        "lstm_model.load_state_dict(torch.load('models/lstm_model.pth'))\n",
        "lstm_model.eval()\n",
        "\n",
        "# Test set predictions\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader_lstm:\n",
        "        X_batch = X_batch.to(device)\n",
        "        outputs = lstm_model(X_batch)\n",
        "        preds = outputs.argmax(1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(y_batch.numpy())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Calculate metrics\n",
        "lstm_acc = accuracy_score(all_labels, all_preds)\n",
        "lstm_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f\"\\nLSTM Test Results:\")\n",
        "print(f\"  Accuracy: {lstm_acc:.4f}\")\n",
        "print(f\"  F1-Score: {lstm_f1:.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title('LSTM Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/confusion_matrix.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Save LSTM metrics\n",
        "lstm_metrics = {\n",
        "    'Accuracy': lstm_acc,\n",
        "    'F1': lstm_f1,\n",
        "    'ConfusionMatrix': cm.tolist()\n",
        "}\n",
        "with open('results/metrics/lstm_metrics.json', 'w') as f:\n",
        "    json.dump(lstm_metrics, f, indent=2)\n",
        "\n",
        "print(\"[OK] Model evaluation complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 11: Results Visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[10/11] RESULTS VISUALIZATION\")\n",
        "\n",
        "# Model comparison\n",
        "comparison_data = {\n",
        "    'Model': ['Logistic Regression', 'Random Forest', 'LSTM'],\n",
        "    'Accuracy': [lr_acc, rf_acc, lstm_acc],\n",
        "    'F1-Score': [lr_f1, rf_f1, lstm_f1]\n",
        "}\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[0].bar(comparison_df['Model'], comparison_df['Accuracy'], color=['blue', 'green', 'red'], alpha=0.7)\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Model Accuracy Comparison', fontweight='bold')\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(comparison_df['Accuracy']):\n",
        "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# F1-Score comparison\n",
        "axes[1].bar(comparison_df['Model'], comparison_df['F1-Score'], color=['blue', 'green', 'red'], alpha=0.7)\n",
        "axes[1].set_ylabel('F1-Score')\n",
        "axes[1].set_title('Model F1-Score Comparison', fontweight='bold')\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(comparison_df['F1-Score']):\n",
        "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/comparison_metrics.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Synthetic data quality comparison\n",
        "synth_comparison = {\n",
        "    'Method': ['Monte Carlo', 'CGAN'],\n",
        "    'JSD': [mc_jsd, cgan_jsd]\n",
        "}\n",
        "synth_df = pd.DataFrame(synth_comparison)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(synth_df['Method'], synth_df['JSD'], color=['orange', 'purple'], alpha=0.7)\n",
        "plt.ylabel('Jensen-Shannon Divergence (lower is better)')\n",
        "plt.title('Synthetic Data Quality Comparison', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(synth_df['JSD']):\n",
        "    plt.text(i, v + 0.005, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/synthetic_quality.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# LSTM predictions vs actual\n",
        "plt.figure(figsize=(14, 6))\n",
        "test_dates = test_df.index[seq_length:][:len(all_preds)]\n",
        "plt.plot(test_dates, all_labels, label='Actual Regime', alpha=0.7, linewidth=2)\n",
        "plt.plot(test_dates, all_preds, label='Predicted Regime', alpha=0.7, linewidth=2, linestyle='--')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Regime (0=bear, 1=bull, 2=volatile)')\n",
        "plt.title('LSTM Regime Predictions vs Actual', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/lstm_predictions.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(\"[OK] Results visualization complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 12: Trading Strategy Backtest\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[11/11] TRADING STRATEGY BACKTEST\")\n",
        "\n",
        "# Simple strategy: Long bull, short bear, neutral volatile\n",
        "test_df_seq = test_df.iloc[seq_length:seq_length+len(all_preds)].copy()\n",
        "test_df_seq['predicted_regime'] = all_preds\n",
        "test_df_seq['actual_regime'] = all_labels\n",
        "\n",
        "# Strategy returns\n",
        "test_df_seq['strategy_signal'] = test_df_seq['predicted_regime'].map({0: -1, 1: 1, 2: 0})  # bear=-1, bull=1, volatile=0\n",
        "test_df_seq['strategy_returns'] = test_df_seq['strategy_signal'].shift(1) * test_df_seq['returns']\n",
        "test_df_seq['cumulative_returns'] = (1 + test_df_seq['returns']).cumprod()\n",
        "test_df_seq['cumulative_strategy'] = (1 + test_df_seq['strategy_returns'].fillna(0)).cumprod()\n",
        "\n",
        "# Calculate Sharpe Ratio\n",
        "strategy_sharpe = test_df_seq['strategy_returns'].mean() / test_df_seq['strategy_returns'].std() * np.sqrt(252)\n",
        "buyhold_sharpe = test_df_seq['returns'].mean() / test_df_seq['returns'].std() * np.sqrt(252)\n",
        "\n",
        "print(f\"\\nTrading Strategy Performance:\")\n",
        "print(f\"  Strategy Sharpe Ratio: {strategy_sharpe:.4f}\")\n",
        "print(f\"  Buy & Hold Sharpe Ratio: {buyhold_sharpe:.4f}\")\n",
        "print(f\"  Total Strategy Return: {(test_df_seq['cumulative_strategy'].iloc[-1] - 1) * 100:.2f}%\")\n",
        "print(f\"  Total Buy & Hold Return: {(test_df_seq['cumulative_returns'].iloc[-1] - 1) * 100:.2f}%\")\n",
        "\n",
        "# Plot cumulative returns\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(test_df_seq.index, test_df_seq['cumulative_returns'], label='Buy & Hold', linewidth=2)\n",
        "plt.plot(test_df_seq.index, test_df_seq['cumulative_strategy'], label='LSTM Strategy', linewidth=2)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Returns')\n",
        "plt.title('Trading Strategy Performance', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/figures/trading_strategy.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Save strategy metrics\n",
        "strategy_metrics = {\n",
        "    'StrategySharpe': strategy_sharpe,\n",
        "    'BuyHoldSharpe': buyhold_sharpe,\n",
        "    'StrategyReturn': float((test_df_seq['cumulative_strategy'].iloc[-1] - 1) * 100),\n",
        "    'BuyHoldReturn': float((test_df_seq['cumulative_returns'].iloc[-1] - 1) * 100)\n",
        "}\n",
        "with open('results/metrics/strategy_metrics.json', 'w') as f:\n",
        "    json.dump(strategy_metrics, f, indent=2)\n",
        "\n",
        "print(\"[OK] Trading strategy backtest complete\")\n",
        "\n",
        "# FINAL SUMMARY\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n[SUMMARY] SUMMARY OF RESULTS:\")\n",
        "print(f\"Baseline Models:\")\n",
        "print(f\"  - ARIMA RMSE: {arima_rmse:.4f}\")\n",
        "print(f\"  - Logistic Regression Accuracy: {lr_acc:.4f}\")\n",
        "print(f\"  - Random Forest Accuracy: {rf_acc:.4f}\")\n",
        "print(f\"\\nDeep Learning Models:\")\n",
        "print(f\"  - CGAN JSD: {cgan_jsd:.4f} (vs Monte Carlo: {mc_jsd:.4f})\")\n",
        "print(f\"  - LSTM Accuracy: {lstm_acc:.4f}\")\n",
        "print(f\"  - LSTM F1-Score: {lstm_f1:.4f}\")\n",
        "print(f\"\\nTrading Strategy:\")\n",
        "print(f\"  - Strategy Sharpe: {strategy_sharpe:.4f}\")\n",
        "print(f\"  - Strategy Return: {(test_df_seq['cumulative_strategy'].iloc[-1] - 1) * 100:.2f}%\")\n",
        "\n",
        "print(\"\\n[FILES] Generated Files:\")\n",
        "print(\"Figures:\")\n",
        "for fig in sorted(Path('results/figures').glob('*.png')):\n",
        "    print(f\"  [OK] {fig}\")\n",
        "print(\"\\nMetrics:\")\n",
        "for metric in sorted(Path('results/metrics').glob('*.json')):\n",
        "    print(f\"  [OK] {metric}\")\n",
        "print(\"\\nModels:\")\n",
        "for model in sorted(Path('models').glob('*')):\n",
        "    print(f\"  [OK] {model}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Next steps:\")\n",
        "print(\"1. Review all figures in results/figures/\")\n",
        "print(\"2. Check metrics in results/metrics/\")\n",
        "print(\"3. Update report.md with Results, Discussion, Conclusion\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}